{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm=ChatGroq(model=\"qwen-2.5-32b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a:int,b:int)-> int:\n",
    "    \"\"\"Add a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools=llm.bind_tools([add])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_call=llm_with_tools.invoke(\"What is 2 plus 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_5689', 'function': {'arguments': '{\"a\": 2, \"b\": 3}', 'name': 'add'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 192, 'total_tokens': 217, 'completion_time': 0.125, 'prompt_time': 0.012240433, 'queue_time': 0.24785531, 'total_time': 0.137240433}, 'model_name': 'qwen-2.5-32b', 'system_fingerprint': 'fp_35f92f8282', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-d2b980b0-1e79-4423-be9c-76a94425fb4e-0', tool_calls=[{'name': 'add', 'args': {'a': 2, 'b': 3}, 'id': 'call_5689', 'type': 'tool_call'}], usage_metadata={'input_tokens': 192, 'output_tokens': 25, 'total_tokens': 217})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_call=llm_with_tools.invoke(\"What is GenAI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='GenAI, short for Generative Artificial Intelligence, refers to a category of AI models that are designed to create new, original content such as text, images, audio, or video. These models are typically trained on large datasets and use techniques like deep learning to generate outputs that are similar to the training data. Examples of GenAI include language models like GPT (Generative Pre-trained Transformer), which can write essays, stories, or even computer code, and image-generating models like DALL-E or Stable Diffusion, which can create digital images or art pieces based on textual descriptions.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 119, 'prompt_tokens': 190, 'total_tokens': 309, 'completion_time': 0.595, 'prompt_time': 0.011121235, 'queue_time': 0.247935348, 'total_time': 0.606121235}, 'model_name': 'qwen-2.5-32b', 'system_fingerprint': 'fp_35f92f8282', 'finish_reason': 'stop', 'logprobs': None}, id='run-94dbbd60-e764-4a6b-a94c-329ed3c8dc52-0', usage_metadata={'input_tokens': 190, 'output_tokens': 119, 'total_tokens': 309})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_call=llm_with_tools.invoke(\"What is 2 multiply by 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The question seems to be asking for a multiplication operation, but the provided tool only supports addition. However, I can directly answer your question without using the tool. \\n\\n2 multiplied by 3 is 6.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 193, 'total_tokens': 236, 'completion_time': 0.215, 'prompt_time': 0.012021191, 'queue_time': 0.28030073, 'total_time': 0.227021191}, 'model_name': 'qwen-2.5-32b', 'system_fingerprint': 'fp_35f92f8282', 'finish_reason': 'stop', 'logprobs': None}, id='run-081e6bbd-38cd-42e2-aee9-18d135391a95-0', usage_metadata={'input_tokens': 193, 'output_tokens': 43, 'total_tokens': 236})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ¤– LangGraph Chatbot (Type 'exit' to quit)\n",
      "\n",
      "\n",
      "--- Full Conversation ---\n",
      "Assistant: Hi! ðŸ‘‹  How can I help you today? ðŸ˜Š\n",
      "\n",
      "\n",
      "--- Full Conversation ---\n",
      "Assistant: **Generative AI (Gen AI)** is a type of artificial intelligence that focuses on creating new content, rather than simply analyzing or understanding existing data. \n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "**What it does:**\n",
      "\n",
      "* **Generates:**  It produces new outputs like text, images, audio, video, code, 3D models, and more. \n",
      "* **Learns from Data:** It's trained on massive datasets of existing content, learning patterns and structures within that data.\n",
      "* **Creates Novel Outputs:** Using its learned knowledge, it can generate original content that is similar in style and substance to the training data, but not simply a copy.\n",
      "\n",
      "**Examples:**\n",
      "\n",
      "* **Text Generation:** ChatGPT (like me!), Bard, Jasper, writing assistants\n",
      "* **Image Generation:** DALL-E 2, Stable Diffusion, Midjourney (creating realistic or artistic images from text prompts)\n",
      "* **Audio Generation:** Jukebox (creating music), AI voice cloning\n",
      "* **Code Generation:** GitHub Copilot (assisting programmers)\n",
      "* **Video Generation:** RunwayML, Synthesia (generating synthetic videos)\n",
      "\n",
      "**How it works:**\n",
      "\n",
      "Gen AI often relies on deep learning algorithms, particularly a type called **Generative Adversarial Networks (GANs)**.  \n",
      "\n",
      "* **GANs** consist of two networks: a generator that creates content and a discriminator that evaluates its authenticity. They compete against each other, improving the generator's ability to create increasingly realistic outputs.\n",
      "\n",
      "**Applications:**\n",
      "\n",
      "Gen AI has a wide range of potential applications:\n",
      "\n",
      "* **Creativity:** Art, music, writing, design\n",
      "* **Productivity:** Automating content creation tasks, code generation\n",
      "* **Entertainment:** Creating interactive stories, generating game assets\n",
      "* **Education:** Personalized learning experiences, generating practice problems\n",
      "* **Research:** Exploring new scientific ideas, simulating complex systems\n",
      "\n",
      "**Ethical Considerations:**\n",
      "\n",
      "* **Bias:** Gen AI models can inherit and amplify biases present in the training data.\n",
      "* **Misinformation:** Potential for generating fake news, deepfakes, and other harmful content.\n",
      "* **Copyright:** Ownership and intellectual property rights of AI-generated content.\n",
      "* **Job displacement:** Automation of creative tasks could impact employment.\n",
      "\n",
      "\n",
      "Gen AI is a rapidly evolving field with immense potential to transform many aspects of our lives. As it continues to develop, it's crucial to address the ethical challenges and ensure responsible and beneficial use.\n",
      "\n",
      "Goodbye! ðŸ‘‹\n"
     ]
    }
   ],
   "source": [
    "# ðŸ“Œ Import Required Libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage, AnyMessage  # âœ… Fix import\n",
    "\n",
    "# âœ… Load Environment Variables\n",
    "load_dotenv()\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# âœ… Define Chat State using TypedDict with `add_messages`\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]  # Stores all chat messages\n",
    "\n",
    "# âœ… Initialize LangGraph Execution Model\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# âœ… Load Groq LLM Model (gemma2-9b-it)\n",
    "llm = ChatGroq(model=\"gemma2-9b-it\")\n",
    "\n",
    "# âœ… Define the Chatbot Function\n",
    "def chatbot(state: State):\n",
    "    \"\"\"\n",
    "    Processes user input and returns AI-generated messages.\n",
    "    \"\"\"\n",
    "    user_input = state[\"messages\"][-1].content  # âœ… Extract latest user message\n",
    "    response = llm.invoke(user_input)  # âœ… Send only the latest message\n",
    "    return {\"messages\": [AIMessage(content=response.content)]}  # âœ… Convert to AIMessage\n",
    "\n",
    "# âœ… Build the AI Graph\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_edge(START, \"chatbot\")  # Start -> chatbot\n",
    "graph_builder.add_edge(\"chatbot\", END)  # chatbot -> End\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# âœ… Function to Stream AI Responses and Retrieve Full Conversation\n",
    "def stream_graph_updates(user_input: str):\n",
    "    \"\"\"\n",
    "    Streams AI-generated responses and prints all messages dynamically.\n",
    "    \"\"\"\n",
    "    for event in graph.stream({\"messages\": [HumanMessage(content=user_input)]}):  # âœ… Use HumanMessage\n",
    "        for value in event.values():\n",
    "            # âœ… Get all messages tracked by LangGraph\n",
    "            all_messages = value[\"messages\"]\n",
    "\n",
    "            # âœ… Print full conversation\n",
    "            print(\"\\n--- Full Conversation ---\")\n",
    "            for msg in all_messages:\n",
    "                role = \"User\" if isinstance(msg, HumanMessage) else \"Assistant\"\n",
    "                print(f\"{role}: {msg.content}\")\n",
    "\n",
    "    for message in state['messages']:\n",
    "        print(message)\n",
    "\n",
    "# âœ… Start Chatbot Interaction\n",
    "def run_chatbot():\n",
    "    \"\"\"\n",
    "    Runs the chatbot in an interactive terminal loop.\n",
    "    \"\"\"\n",
    "    print(\"\\nðŸ¤– LangGraph Chatbot (Type 'exit' to quit)\\n\")\n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"User: \")\n",
    "            if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "                print(\"Goodbye! ðŸ‘‹\")\n",
    "                break\n",
    "\n",
    "            stream_graph_updates(user_input)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nSession ended. Goodbye! ðŸ‘‹\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "# âœ… Run the Chatbot\n",
    "if __name__ == \"__main__\":\n",
    "    run_chatbot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'CompiledStateGraph' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mgraph\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'CompiledStateGraph' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "def display(state):\n",
    "    for message in state['messages']:\n",
    "        print(message)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ¤– LangGraph Chatbot (Type 'exit' to quit)\n",
      "\n",
      "\n",
      "--- Full Conversation ---\n",
      "Assistant: Hello! ðŸ‘‹  How can I help you today?\n",
      "\n",
      "\n",
      "--- Full Conversation ---\n",
      "User: hi\n",
      "Assistant: Hello! ðŸ‘‹  How can I help you today?\n",
      "\n",
      "\n",
      "--- Full Conversation ---\n",
      "User: hi\n",
      "Assistant: Hello! ðŸ‘‹  How can I help you today?\n",
      "\n",
      "\n",
      "--- Full Conversation ---\n",
      "Assistant: GenAI, or Generative Artificial Intelligence, is a type of artificial intelligence that focuses on creating new content. \n",
      "\n",
      "Think of it like this:\n",
      "\n",
      "* **Traditional AI** is good at analyzing existing data and making predictions. For example, it can analyze your past spending habits to predict what you might buy next.\n",
      "* **GenAI** goes a step further. It learns patterns and structures from existing data and uses that knowledge to generate brand new content. This could be anything from text and code to images, music, and even videos.\n",
      "\n",
      "Here are some key things to know about GenAI:\n",
      "\n",
      "* **How it works:** GenAI models are trained on massive datasets of text, code, images, or other types of data. This allows them to learn the underlying patterns and structures within that data.\n",
      "* **Examples:** Some popular examples of GenAI include:\n",
      "    * **Text generation:** ChatGPT, Jasper, Bard\n",
      "    * **Image generation:** DALL-E 2, Stable Diffusion, Midjourney\n",
      "    * **Code generation:** GitHub Copilot\n",
      "* **Applications:** GenAI has a wide range of potential applications, including:\n",
      "    * **Content creation:** Writing articles, stories, poems, marketing copy, and more\n",
      "    * **Art and design:** Generating new artworks, logos, and designs\n",
      "    * **Software development:** Automating code generation and debugging\n",
      "    * **Education:** Creating personalized learning experiences\n",
      "    * **Research:** Accelerating scientific discovery\n",
      "\n",
      "* **Ethical considerations:** Like all powerful technologies, GenAI raises ethical concerns, such as:\n",
      "    * **Bias and fairness:** GenAI models can inherit and amplify biases present in the training data.\n",
      "    * **Misinformation and manipulation:** GenAI can be used to create realistic fake content, which can be used for malicious purposes.\n",
      "    * **Job displacement:** GenAI may automate tasks currently performed by humans, potentially leading to job losses.\n",
      "\n",
      "It's important to use GenAI responsibly and ethically.\n",
      "\n",
      "As a large language model myself, I am a product of GenAI. I can help you learn more about this exciting field and explore its potential applications.\n",
      "\n",
      "\n",
      "--- Full Conversation ---\n",
      "User: what is genai\n",
      "Assistant: GenAI, or Generative Artificial Intelligence, is a type of artificial intelligence that focuses on creating new content. \n",
      "\n",
      "Think of it like this:\n",
      "\n",
      "* **Traditional AI** is good at analyzing existing data and making predictions. For example, it can analyze your past spending habits to predict what you might buy next.\n",
      "* **GenAI** goes a step further. It learns patterns and structures from existing data and uses that knowledge to generate brand new content. This could be anything from text and code to images, music, and even videos.\n",
      "\n",
      "Here are some key things to know about GenAI:\n",
      "\n",
      "* **How it works:** GenAI models are trained on massive datasets of text, code, images, or other types of data. This allows them to learn the underlying patterns and structures within that data.\n",
      "* **Examples:** Some popular examples of GenAI include:\n",
      "    * **Text generation:** ChatGPT, Jasper, Bard\n",
      "    * **Image generation:** DALL-E 2, Stable Diffusion, Midjourney\n",
      "    * **Code generation:** GitHub Copilot\n",
      "* **Applications:** GenAI has a wide range of potential applications, including:\n",
      "    * **Content creation:** Writing articles, stories, poems, marketing copy, and more\n",
      "    * **Art and design:** Generating new artworks, logos, and designs\n",
      "    * **Software development:** Automating code generation and debugging\n",
      "    * **Education:** Creating personalized learning experiences\n",
      "    * **Research:** Accelerating scientific discovery\n",
      "\n",
      "* **Ethical considerations:** Like all powerful technologies, GenAI raises ethical concerns, such as:\n",
      "    * **Bias and fairness:** GenAI models can inherit and amplify biases present in the training data.\n",
      "    * **Misinformation and manipulation:** GenAI can be used to create realistic fake content, which can be used for malicious purposes.\n",
      "    * **Job displacement:** GenAI may automate tasks currently performed by humans, potentially leading to job losses.\n",
      "\n",
      "It's important to use GenAI responsibly and ethically.\n",
      "\n",
      "As a large language model myself, I am a product of GenAI. I can help you learn more about this exciting field and explore its potential applications.\n",
      "\n",
      "\n",
      "--- Full Conversation ---\n",
      "User: what is genai\n",
      "Assistant: GenAI, or Generative Artificial Intelligence, is a type of artificial intelligence that focuses on creating new content. \n",
      "\n",
      "Think of it like this:\n",
      "\n",
      "* **Traditional AI** is good at analyzing existing data and making predictions. For example, it can analyze your past spending habits to predict what you might buy next.\n",
      "* **GenAI** goes a step further. It learns patterns and structures from existing data and uses that knowledge to generate brand new content. This could be anything from text and code to images, music, and even videos.\n",
      "\n",
      "Here are some key things to know about GenAI:\n",
      "\n",
      "* **How it works:** GenAI models are trained on massive datasets of text, code, images, or other types of data. This allows them to learn the underlying patterns and structures within that data.\n",
      "* **Examples:** Some popular examples of GenAI include:\n",
      "    * **Text generation:** ChatGPT, Jasper, Bard\n",
      "    * **Image generation:** DALL-E 2, Stable Diffusion, Midjourney\n",
      "    * **Code generation:** GitHub Copilot\n",
      "* **Applications:** GenAI has a wide range of potential applications, including:\n",
      "    * **Content creation:** Writing articles, stories, poems, marketing copy, and more\n",
      "    * **Art and design:** Generating new artworks, logos, and designs\n",
      "    * **Software development:** Automating code generation and debugging\n",
      "    * **Education:** Creating personalized learning experiences\n",
      "    * **Research:** Accelerating scientific discovery\n",
      "\n",
      "* **Ethical considerations:** Like all powerful technologies, GenAI raises ethical concerns, such as:\n",
      "    * **Bias and fairness:** GenAI models can inherit and amplify biases present in the training data.\n",
      "    * **Misinformation and manipulation:** GenAI can be used to create realistic fake content, which can be used for malicious purposes.\n",
      "    * **Job displacement:** GenAI may automate tasks currently performed by humans, potentially leading to job losses.\n",
      "\n",
      "It's important to use GenAI responsibly and ethically.\n",
      "\n",
      "As a large language model myself, I am a product of GenAI. I can help you learn more about this exciting field and explore its potential applications.\n",
      "\n",
      "Goodbye! ðŸ‘‹\n"
     ]
    }
   ],
   "source": [
    "# ðŸ“Œ Import Required Libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage, AnyMessage\n",
    "\n",
    "# âœ… Load Environment Variables\n",
    "load_dotenv()\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# âœ… Define Chat State using TypedDict with `add_messages`\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]  # Stores all chat messages\n",
    "\n",
    "# âœ… Initialize LangGraph Execution Model\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# âœ… Load Groq LLM Model (gemma2-9b-it)\n",
    "llm = ChatGroq(model=\"gemma2-9b-it\")\n",
    "\n",
    "# âœ… Define the Chatbot Function\n",
    "def chatbot(state: State):\n",
    "    \"\"\"\n",
    "    Processes user input and returns AI-generated messages.\n",
    "    \"\"\"\n",
    "    user_input = state[\"messages\"][-1].content  # Extract latest user message\n",
    "    response = llm.invoke(user_input)  # Get AI response\n",
    "    return {\"messages\": [AIMessage(content=response.content)]}  # Convert to AIMessage\n",
    "\n",
    "\n",
    "# âœ… Define the Function to Display the Full Conversation\n",
    "def display(state: State):\n",
    "    \"\"\"\n",
    "    Displays the entire conversation history.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Full Conversation ---\")\n",
    "    for message in state[\"messages\"]:\n",
    "        role = \"User\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "        print(f\"{role}: {message.content}\")\n",
    "    return state  # Return state to continue execution\n",
    "\n",
    "\n",
    "# âœ… Build the AI Graph\n",
    "graph_builder.add_node(\"chatbot\", chatbot)  # AI response node\n",
    "graph_builder.add_node(\"display\", display)  # Display node\n",
    "\n",
    "# âœ… Define Graph Execution Order\n",
    "graph_builder.add_edge(START, \"chatbot\")  # Start -> chatbot\n",
    "graph_builder.add_edge(\"chatbot\", \"display\")  # chatbot -> display\n",
    "graph_builder.add_edge(\"display\", END)  # display -> End\n",
    "\n",
    "# âœ… Compile the Graph\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "def stream_graph_updates(user_input: str):\n",
    "    \"\"\"\n",
    "    Streams AI-generated responses and prints all messages dynamically.\n",
    "    \"\"\"\n",
    "    \n",
    "    for event in graph.stream({\"messages\": [HumanMessage(content=user_input)]}):  # âœ… Use HumanMessage\n",
    "        for value in event.values():\n",
    "            # âœ… Get all messages tracked by LangGraph\n",
    "            all_messages = value[\"messages\"]\n",
    "\n",
    "            # âœ… Print full conversation\n",
    "            print(\"\\n--- Full Conversation ---\")\n",
    "            for msg in all_messages:\n",
    "                role = \"User\" if isinstance(msg, HumanMessage) else \"Assistant\"\n",
    "                print(f\"{role}: {msg.content}\")\n",
    "\n",
    "\n",
    "# âœ… Start Chatbot Interaction\n",
    "def run_chatbot():\n",
    "    \"\"\"\n",
    "    Runs the chatbot in an interactive terminal loop.\n",
    "    \"\"\"\n",
    "    print(\"\\nðŸ¤– LangGraph Chatbot (Type 'exit' to quit)\\n\")\n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"User: \")\n",
    "            if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "                print(\"Goodbye! ðŸ‘‹\")\n",
    "                break\n",
    "\n",
    "            stream_graph_updates(user_input)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nSession ended. Goodbye! ðŸ‘‹\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "# âœ… Run the Chatbot\n",
    "if __name__ == \"__main__\":\n",
    "    run_chatbot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'AIMessage' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 62\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# âœ… Compile the Graph\u001b[39;00m\n\u001b[0;32m     60\u001b[0m graph \u001b[38;5;241m=\u001b[39m graph_builder\u001b[38;5;241m.\u001b[39mcompile()\n\u001b[1;32m---> 62\u001b[0m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstart\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstart\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\miniconda3\\envs\\agenticai_env\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2142\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[0;32m   2140\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2141\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 2142\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2143\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2146\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2147\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2148\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2150\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2151\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   2152\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[0;32m   2153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\miniconda3\\envs\\agenticai_env\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1797\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[0;32m   1791\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[0;32m   1792\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[0;32m   1793\u001b[0m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[0;32m   1794\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[0;32m   1795\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[0;32m   1796\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[1;32m-> 1797\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1798\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1799\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1800\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1801\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1802\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1803\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[0;32m   1804\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1805\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\miniconda3\\envs\\agenticai_env\\Lib\\site-packages\\langgraph\\pregel\\runner.py:230\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[0;32m    228\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 230\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_SEND\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\DELL\\miniconda3\\envs\\agenticai_env\\Lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[0;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[1;32mc:\\Users\\DELL\\miniconda3\\envs\\agenticai_env\\Lib\\site-packages\\langgraph\\utils\\runnable.py:546\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    542\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[0;32m    543\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    544\u001b[0m )\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 546\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    548\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\miniconda3\\envs\\agenticai_env\\Lib\\site-packages\\langgraph\\utils\\runnable.py:310\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m--> 310\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[1;32mIn[22], line 34\u001b[0m, in \u001b[0;36mchatbot\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     32\u001b[0m     state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39mllm\u001b[38;5;241m.\u001b[39minvoke(user_input)\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m:state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n",
      "\u001b[1;31mTypeError\u001b[0m: 'AIMessage' object is not subscriptable",
      "\u001b[0mDuring task with name 'chatbot' and id '7703112c-9540-e752-6003-8866019a265d'"
     ]
    }
   ],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "llm=ChatGroq(model=\"gemma2-9b-it\")\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    start:str\n",
    "    messages:Annotated[list,add_messages]\n",
    "\n",
    "\n",
    "\n",
    "def chatbot(state:State):\n",
    "    while True:\n",
    "    \n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        state[\"messages\"]=llm.invoke(user_input)\n",
    "        print(f'{user_input}\\n')\n",
    "        print(state['messages'][-1].content)\n",
    "    return {\"messages\":state[\"messages\"]}\n",
    "\n",
    "def display(state: State):\n",
    "    \"\"\"\n",
    "    Displays the entire conversation history.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Full Conversation ---\")\n",
    "    for message in state[\"messages\"]:\n",
    "        print(message)\n",
    "    return {'messages':state['messages']} # Return state to continue execution\n",
    "\n",
    "graph_builder=StateGraph(State)\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Build the AI Graph\n",
    "graph_builder.add_node(\"chatbot\", chatbot)  # AI response node\n",
    "graph_builder.add_node(\"display\", display)  # Display node\n",
    "\n",
    "# âœ… Define Graph Execution Order\n",
    "graph_builder.add_edge(START, \"chatbot\")  # Start -> chatbot\n",
    "graph_builder.add_edge(\"chatbot\", \"display\")  # chatbot -> display\n",
    "graph_builder.add_edge(\"display\", END)  # display -> End\n",
    "\n",
    "# âœ… Compile the Graph\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "graph.invoke({'start':'start'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agenticai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
